{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc96HGV0pYkJ"
   },
   "source": [
    "# Introduction to Data Science 2025\n",
    "\n",
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWh-qfRJpYkK"
   },
   "source": [
    "## Exercise 1 | Titanic: data preprocessing and imputation\n",
    "<span style=\"font-weight: bold\"> *Note: You can find tutorials for NumPy and Pandas under 'Useful tutorials' in the course material.*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZy31vJxpYkK"
   },
   "source": [
    "Download the [Titanic dataset](https://www.kaggle.com/c/titanic) [train.csv] from Kaggle or <span style=\"font-weight: 500\">directly from the course material</span>, and complete the following exercises. If you choose to download the dataset from Kaggle, you will need to create a Kaggle account unless you already have one, but it is quite straightforward.\n",
    "\n",
    "The dataset consists of personal information of all the passengers on board the RMS Titanic, along with information about whether they survived the iceberg collision or not.\n",
    "\n",
    "1. Your first task is to read the data file and print the shape of the data.\n",
    "\n",
    "    <span style=\"font-weight: 500\"> *Hint 1: You can read them into a Pandas dataframe if you wish.*</span>\n",
    "    \n",
    "    <span style=\"font-weight: 500\"> *Hint 2: The shape of the data should be (891, 12).*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l4kCNDespYkK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCZoOS-wpYkL"
   },
   "source": [
    "2. Let's look at the data and get started with some preprocessing. Some of the columns, e.g <span style=\"font-weight: 500\"> *Name*</span>, simply identify a person and are not useful for prediction tasks. Try to identify these columns, and remove them.\n",
    "\n",
    "    <span style=\"font-weight: 500\"> *Hint: The shape of the data should now be (891, 9).*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "m18V-jpTpYkL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Name', 'Sex', 'PassengerId'], axis=1, inplace=True)\n",
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48AcPDOrpYkL"
   },
   "source": [
    "3. The column <span style=\"font-weight: 500\">*Cabin*</span> contains a letter and a number. A smart catch at this point would be to notice that the letter stands for the deck level on the ship. Keeping just the deck information would be more informative when developing, e.g. a classifier that predicts whether a passenger survived. The next step in our preprocessing will be to add a new column to the dataset, which consists simply of the deck letter. You can then remove the original <span style=\"font-weight: 500\">*Cabin*</span>-column.\n",
    "\n",
    "<span style=\"font-weight: 500\">*Hint: The deck letters should be ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'T'].*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQX5LmWrpYkL"
   },
   "outputs": [],
   "source": [
    "df['Deck'] = df['Cabin'].str[0]\n",
    "df.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mupSH5tXpYkL"
   },
   "source": [
    "4. You’ll notice that some of the columns, such as the previously added deck number, are [categorical](https://en.wikipedia.org/wiki/Categorical_variable). To preprocess the categorical variables so that they're ready for further computation, we need to avoid the current string format of the values. This means the next step for each categorical variable is to transform the string values to numeric ones, that correspond to a unique integer ID representative of each distinct category. This process is called label encoding and you can read more about it [here](https://pandas.pydata.org/docs/user_guide/categorical.html).\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint: Pandas can do this for you.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cfb0_uzRpYkL"
   },
   "outputs": [],
   "source": [
    "df['Deck'] = df['Deck'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32_hHxGLpYkL"
   },
   "source": [
    "5. Next, let's look into missing value **imputation**. Some of the rows in the data have missing values, e.g when the cabin number of a person is unknown. Most machine learning algorithms have trouble with missing values, and they need to be handled during preprocessing:\n",
    "\n",
    "    a) For continuous variables, replace the missing values with the mean of the non-missing values of that column.\n",
    "\n",
    "    b) For categorical variables, replace the missing values with the mode of the column.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Remember: Even though in the previous step we transformed categorical variables into their numeric representation, they are still categorical.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "e0kh2bbGpYkL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "for col in df.select_dtypes(include=['object', 'category']):\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15Kbmgx9pYkM"
   },
   "source": [
    "6. At this point, all data is numeric. Write the data, with the modifications we made, to a  <span style=\"font-weight: 500\"> .csv</span> file. Then, write another file, this time in <span style=\"font-weight: 500\">JSON</span> format, with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_EhC78NpYkM"
   },
   "outputs": [],
   "source": [
    "#[\n",
    "#    {\n",
    "#        \"Deck\": 0,\n",
    "#        \"Age\": 20,\n",
    "#        \"Survived\", 0\n",
    "#        ...\n",
    "#    },\n",
    "#    {\n",
    "#        ...\n",
    "#    }\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lxF-ehbapYkM"
   },
   "outputs": [],
   "source": [
    "df.to_json('train.json', orient='records', compression='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnkAgzHjpYkM"
   },
   "source": [
    "Study the records and try to see if there is any evident pattern in terms of chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddqs0UqLpYkM"
   },
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGnzPePKpYkM"
   },
   "source": [
    "## Exercise 2 | Titanic 2.0: exploratory data analysis\n",
    "\n",
    "In this exercise, we’ll continue to study the Titanic dataset from the last exercise. Now that we have done some preprocessing, it’s time to look at the data with some exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5nTB9ExpYkM"
   },
   "source": [
    "1. First investigate each feature variable in turn. For each categorical variable, find out the mode, i.e., the most frequent value. For numerical variables, calculate the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hKdMpHZApYkM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived     0.383838\n",
      "Pclass       2.308642\n",
      "Age         29.699118\n",
      "SibSp        0.523008\n",
      "Parch        0.381594\n",
      "Fare        32.204208\n",
      "dtype: float64\n",
      "Ticket      1601\n",
      "Embarked       S\n",
      "Deck           C\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.select_dtypes(include=\"number\").mean())\n",
    "print(df.select_dtypes(exclude=\"number\").mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOjLqfDkpYkM"
   },
   "source": [
    "2. Next, combine the modes of the categorical variables, and the medians of the numerical variables, to construct an imaginary “average survivor”. This \"average survivor\" should represent the typical passenger of the class of passengers who survived. Also following the same principle, construct the “average non-survivor”.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint 1: What are the average/most frequent variable values for a non-survivor?*</span>\n",
    "    \n",
    "    <span style=\"font-weight: 500\">*Hint 2: You can split the dataframe in two: one subset containing all the survivors and one consisting of all the non-survivor instances. Then, you can use the summary statistics of each of these dataframe to create a prototype \"average survivor\" and \"average non-survivor\", respectively.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OhUU2GIDpYkM"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not convert ['PC 17599STON/O2. 3101282113803347742237736PP 95491137832487062443732649248698330923113788347077330959PC 1756933567726772651SC/Paris 212333095814311PC 17572292619947C.A. 31026C.A. 346511135722661C.A. 2939531012811601248738364516345779330932SO/C 14885310127819950PC 17759231919343120312991371110272672651C 17369266822841411752347081STON/O2. 310127935004311377635851C.A. 33595363291113505347742230136315153370365111428234604350046230080PC 17610PC 17569370370A/5 35402699367231112277F.C.C. 1352835273STON/O2. 310128311813SOTON/OQ 39208919943SW/PP 75136973347077C.A. 31921367230250649117512650PC 17585110152PC 17755230433347077347083PC 17582PC 17760LINE25064437037513502C.A. 2673A/5. 10482345774237798370373198771196719988PC 17558923436722622659311378117421PC 17758PC 1748511767PC 176083474702443673692816966234818248738PC 1776028551363291111361367226PC 176111696675982300801995024873331418386525C.A. 37671113505330931330980110813262614313PC 1747711765PC 17604C 7077PC 17757265311378927849SC 174811376035003435273PP 9549240929STON/O 2. 31012892910619928STON/O 2. 3101269282202506522003250655SOTON/O.Q. 392078110564376564SC/AH 308511376029106F.C.C. 13529230434653063363825064411379426661137861745334924017464F.C.C. 13531199522666C.A. 34651SC/AH Basle 541310129841341196719943C.A. 3767136947110152263601114271601382651PC 17473PC 17603C.A. 3426022687512749111361W./C. 14258263602668F.C.C. 13529PC 1776113568WE/P 573529082908SC/PARIS 2146C.A. 3311217421262034708511755110413345572350417S.W./PP 75211769PC 174741431224388013507STON/O 2. 3101286237789174211104131356736947248727PC 17485243847111426113804SC/Paris 2123PC 17582367230220845230136117532653135022704213214237668PC 1747716012666PC 1757213213CA. 2314231919330919PC 17611STON/O 2. 310128519996297502442704138PC 17755PC 175722416017474349256160135852PC 17757PC 17475223596PC 174761137812661PC 1748219996PC 1775734218248727113806310273686624160PC 1775516988PC 17608STON/O 2. 31012882506482910339209622084525064911015226631137601350229105364516241602687174743101265C.A. 231517465349244C.A. 31921113760262534708911380612749315098392096S.C./PARIS 2079367228113572265929106PC 1775616011177411379817453PC 1759239209136928113055266617466236852SC/PARIS 214934774211751P/PP 3381266711767230433112053111369'\n 'CSSSCSSSSCSQSSQCQCCCQQCSSSSSCSSSSSSQSSSCSSSQSCSCSSSSSSQSSSSSSQSSSSCCQSCQCSCSCSSSSSSQSSCCSCSSSSSSSQSSSSSQSCSCQQQSCCCCCSSSCQSCSSCQSCSSSSSSSSQQCCQCCCSCCSSCSSCSSSSQSSSSSSSSSSSSSSSSSCSCCSSSCSCSSCSSCSSSSQSCSSSCSSCSCCSSSCSCCSCSSSSSSQSSSSCSCCSCSCSCSQSSSCSSCSCSCCCSSQSSSSSSCCSSCSQCSSSSCCSCSSSSQSCSCSSSSSSSCSSSSSCSSSSSSSCSSSSSCQSCSCSCCCSSSSCSSCSSCCCSSC'\n 'CCCCCGCCCCDCACCBCCCCCCDCCCCBCFCCCCCCCCCDCCCCECCFCDCCCCCCCECFCCCCCFBBCCCCACDCDCCCCCCCCDCCBCCCCCCCCCDCCCCCCBCBCCECCCECBCCCECCCDCBCCECFCFCCCECCDCCBECCCCCCCBCDGCCCCCCCCCECCCBCCCCACCCCCCDCECCDCCBCCDBCCCCECFCBBCCCCCBBCCCCCCCAECCCCECCECCCEBDCACCCCCCCFDCDABCBCCDACCCCCCCCCBDBBCCCCECECCCCCECECCBBDBCCCECCBCBDCCBCBCCDCCBCCEBCECCBCCECCCCDCCECDCCCDCCCCBC'] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m survivors = df[df[\u001b[33m\"\u001b[39m\u001b[33mSurvived\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m1\u001b[39m]\n\u001b[32m      2\u001b[39m rip = df[df[\u001b[33m\"\u001b[39m\u001b[33mSurvived\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43msurvivors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/frame.py:11700\u001b[39m, in \u001b[36mDataFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11692\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  11694\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11698\u001b[39m     **kwargs,\n\u001b[32m  11699\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11700\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11701\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11702\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/generic.py:12439\u001b[39m, in \u001b[36mNDFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12432\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  12433\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12434\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12437\u001b[39m     **kwargs,\n\u001b[32m  12438\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12440\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12441\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/generic.py:12396\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12392\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12394\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12398\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/frame.py:11569\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11565\u001b[39m     df = df.T\n\u001b[32m  11567\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11568\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11569\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11570\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1498\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1503\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/internals/blocks.py:406\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    409\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/frame.py:11488\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11486\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    718\u001b[39m count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n\u001b[32m    719\u001b[39m the_sum = values.sum(axis, dtype=dtype_sum)\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m the_sum = \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    723\u001b[39m     count = cast(np.ndarray, count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/pandas/core/nanops.py:1686\u001b[39m, in \u001b[36m_ensure_numeric\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1683\u001b[39m inferred = lib.infer_dtype(x)\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inferred \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1686\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to numeric\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1688\u001b[39m     x = x.astype(np.complex128)\n",
      "\u001b[31mTypeError\u001b[39m: Could not convert ['PC 17599STON/O2. 3101282113803347742237736PP 95491137832487062443732649248698330923113788347077330959PC 1756933567726772651SC/Paris 212333095814311PC 17572292619947C.A. 31026C.A. 346511135722661C.A. 2939531012811601248738364516345779330932SO/C 14885310127819950PC 17759231919343120312991371110272672651C 17369266822841411752347081STON/O2. 310127935004311377635851C.A. 33595363291113505347742230136315153370365111428234604350046230080PC 17610PC 17569370370A/5 35402699367231112277F.C.C. 1352835273STON/O2. 310128311813SOTON/OQ 39208919943SW/PP 75136973347077C.A. 31921367230250649117512650PC 17585110152PC 17755230433347077347083PC 17582PC 17760LINE25064437037513502C.A. 2673A/5. 10482345774237798370373198771196719988PC 17558923436722622659311378117421PC 17758PC 1748511767PC 176083474702443673692816966234818248738PC 1776028551363291111361367226PC 176111696675982300801995024873331418386525C.A. 37671113505330931330980110813262614313PC 1747711765PC 17604C 7077PC 17757265311378927849SC 174811376035003435273PP 9549240929STON/O 2. 31012892910619928STON/O 2. 3101269282202506522003250655SOTON/O.Q. 392078110564376564SC/AH 308511376029106F.C.C. 13529230434653063363825064411379426661137861745334924017464F.C.C. 13531199522666C.A. 34651SC/AH Basle 541310129841341196719943C.A. 3767136947110152263601114271601382651PC 17473PC 17603C.A. 3426022687512749111361W./C. 14258263602668F.C.C. 13529PC 1776113568WE/P 573529082908SC/PARIS 2146C.A. 3311217421262034708511755110413345572350417S.W./PP 75211769PC 174741431224388013507STON/O 2. 3101286237789174211104131356736947248727PC 17485243847111426113804SC/Paris 2123PC 17582367230220845230136117532653135022704213214237668PC 1747716012666PC 1757213213CA. 2314231919330919PC 17611STON/O 2. 310128519996297502442704138PC 17755PC 175722416017474349256160135852PC 17757PC 17475223596PC 174761137812661PC 1748219996PC 1775734218248727113806310273686624160PC 1775516988PC 17608STON/O 2. 31012882506482910339209622084525064911015226631137601350229105364516241602687174743101265C.A. 231517465349244C.A. 31921113760262534708911380612749315098392096S.C./PARIS 2079367228113572265929106PC 1775616011177411379817453PC 1759239209136928113055266617466236852SC/PARIS 214934774211751P/PP 3381266711767230433112053111369'\n 'CSSSCSSSSCSQSSQCQCCCQQCSSSSSCSSSSSSQSSSCSSSQSCSCSSSSSSQSSSSSSQSSSSCCQSCQCSCSCSSSSSSQSSCCSCSSSSSSSQSSSSSQSCSCQQQSCCCCCSSSCQSCSSCQSCSSSSSSSSQQCCQCCCSCCSSCSSCSSSSQSSSSSSSSSSSSSSSSSCSCCSSSCSCSSCSSCSSSSQSCSSSCSSCSCCSSSCSCCSCSSSSSSQSSSSCSCCSCSCSCSQSSSCSSCSCSCCCSSQSSSSSSCCSSCSQCSSSSCCSCSSSSQSCSCSSSSSSSCSSSSSCSSSSSSSCSSSSSCQSCSCSCCCSSSSCSSCSSCCCSSC'\n 'CCCCCGCCCCDCACCBCCCCCCDCCCCBCFCCCCCCCCCDCCCCECCFCDCCCCCCCECFCCCCCFBBCCCCACDCDCCCCCCCCDCCBCCCCCCCCCDCCCCCCBCBCCECCCECBCCCECCCDCBCCECFCFCCCECCDCCBECCCCCCCBCDGCCCCCCCCCECCCBCCCCACCCCCCDCECCDCCBCCDBCCCCECFCBBCCCCCBBCCCCCCCAECCCCECCECCCEBDCACCCCCCCFDCDABCBCCDACCCCCCCCCBDBBCCCCECECCCCCECECCBBDBCCCECCBCBDCCBCBCCDCCBCCEBCECCBCCECCCCDCCECDCCCDCCCCBC'] to numeric"
     ]
    }
   ],
   "source": [
    "survivors = df[df[\"Survived\"] == 1]\n",
    "rip = df[df[\"Survived\"] == 0]\n",
    "print(survivors.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3Bax_hlpYkM"
   },
   "source": [
    "3. Next, let's study the distributions of the variables in the two groups (survivor/non-survivor). How well do the average cases represent the respective groups? Can you find actual passengers that are very similar to the (average) representative of their own group? Can you find passengers that are very similar to the (average) representative of the other group?\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Note: Feel free to choose EDA methods according to your preference: non-graphical/graphical, static/interactive - anything goes.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNsSMXRMpYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc96MC9CpYkM"
   },
   "source": [
    "4. Next, let's continue the analysis by looking into pairwise and multivariate relationships between the variables in the two groups. Try to visualize two variables at a time using, e.g., scatter plots and use a different color to encode the survival status.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint 1: You can also check out Seaborn's pairplot function, if you wish.*</span>\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint 2: To better show many data points with the same value for a given variable, you can use either transparency or ‘jitter’.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fexBqTMQpYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNOrCjKzpYkM"
   },
   "source": [
    "5. Finally, recall the preprocessing we did in the first exercise. What can you say about the effect of the choices that were made to use the mode and mean to impute missing values, instead of, for example, ignoring passengers with missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8TeRhWapYkM"
   },
   "source": [
    "*Use this (markdown) cell for your written answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsrhB5wvpYkM"
   },
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqgXjyklpYkM"
   },
   "source": [
    "## Exercise 3 | Working with text data 2.0\n",
    "\n",
    "This exercise is related to the second exercise from last week. Find the saved <span style=\"font-weight: 500\">pos.txt</span> and <span style=\"font-weight: 500\">neg.txt</span> files, or, alternatively, you can find the week 1 example solutions on the MOOC platform after Tuesday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-DfVlSWpYkM"
   },
   "source": [
    "1. Find the most common words in each file (positive and negative). Examine the results. Do they tend to be general terms relating to the nature of the data? How well do they indicate positive/negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDsK6jXMpYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlU_trsrpYkM"
   },
   "source": [
    "2. Compute a [TF/IDF](https://en.wikipedia.org/wiki/Tf–idf) vector for each of the two text files, and make them into a <span style=\"font-weight: 500\">2 x m</span> matrix, where <span style=\"font-weight: 500\">m</span> is the number of unique words in the data. The problem with using the most common words in a review to analyze its contents is that words that are common overall will be common in all reviews (both positive and negative). This means that they probably are not good indicators about the sentiment of a specific review. TF/IDF stands for Term Frequency / Inverse Document Frequency (here the reviews are the documents), and is designed to help by taking into consideration not just the number of times a term occurs (term frequency), but also how many times a word exists in other reviews as well (inverse document frequency). You can use any variant of the formula, as well as off-the-shelf implementations. <span style=\"font-weight: 500\">*Hint: You can use [sklearn](http://scikit-learn.org/).*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt_t-Lx8pYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lsOuLeOpYkM"
   },
   "source": [
    "3. List the words with the highest TF/IDF score in each class (positive | negative), and compare them to the most common words. What do you notice? Did TF/IDF work as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1tkcbH5pYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z_vWhnXpYkM"
   },
   "source": [
    "4. Plot the words in each class with their corresponding TF/IDF scores. Note that there will be a lot of words, so you’ll have to think carefully to make your chart clear! If you can’t plot them all, plot a subset – think about how you should choose this subset.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint: you can use word clouds. But feel free to challenge yourselves to think of any other meaningful way to visualize this information!*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eUJ3HxlpYkN"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8-yzV5HpYkN"
   },
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_Xd1n98pYkQ"
   },
   "source": [
    "## Exercise 4 | Junk charts\n",
    "\n",
    "There’s a thriving community of chart enthusiasts who keep looking for statistical graphics that they find inappropriate, and which they call “junk charts”, and who often also propose ways to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w78AogNWpYkQ"
   },
   "source": [
    "1. Find at least three statistical visualizations you think are not very good and identify their problems. Copying examples from various junk chart websites is not accepted – you should find your own junk charts, out in the wild. You should be able to find good (or rather, bad) examples quite easily since a significant fraction of charts can have at least *some* issues. The examples you choose should also have different problems, e.g., try to avoid collecting three bar charts, all with problematic axes. Instead, try to find as interesting and diverse examples as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJfbaQH2pYkQ"
   },
   "source": [
    "2. Try to produce improved versions of the charts you selected. The data is of course often not available, but perhaps you can try to extract it, at least approximately, from the chart. Or perhaps you can simulate data that looks similar enough to make the point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-rwa7IrpYkQ"
   },
   "source": [
    "**Submit a PDF with all the charts (the ones you found and the ones you produced).**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
